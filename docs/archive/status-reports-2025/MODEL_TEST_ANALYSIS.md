# Jetson Model Testing Analysis
**Date:** 2025-11-18
**Hardware:** NVIDIA Jetson (16GB RAM, Tegra ARM64)

## Test Summary

All models tested through actual Zoe API with multi-turn conversations requiring context memory.

### Models Tested

| Model | Size | Avg Time | Quality | Hallucinations | Context Lost | Verdict |
|-------|------|----------|---------|----------------|--------------|---------|
| **Llama-3.2-3B** | 1.9GB | **3.19s** | 75/100 | 0 | 1/3 | ‚≠ê **RECOMMENDED** |
| **Qwen2.5-7B Q4** | 4.4GB | **3.26s** | 75/100 | 0 | 1/3 | ‚≠ê **RECOMMENDED** |
| **Qwen2.5-7B Q3** | 3.6GB | 7.56s | 75/100 | 0 | 1/3 | ‚ùå Too Slow |
| **SmolLM2-1.7B** | 1.0GB | 1.5s | 40/100 | Yes | 2/3 | ‚ùå Unreliable |

### Test Conversations

1. **Greeting Test (2 turns)**
   - Simple warmup to check basic coherence
   - All models passed ‚úì

2. **Party Planning Test (4 turns with memory)**
   - Tests context retention across turns
   - "Planning birthday party for Sarah next weekend"
   - "What should I put on shopping list?"
   - "How much will that cost?"
   - **"Remind me what we're planning?"** ‚Üê Critical memory test
   
3. **Joke Test (2 turns)**
   - Tests conversational flow
   - All models passed ‚úì

## Key Findings

### ‚úÖ Top Performers (TIE)

**1. Llama-3.2-3B (1.9GB)**
- **Speed:** 3.19s average
- **Quality:** No hallucinations, coherent responses
- **Memory:** Good but not perfect (forgot party context in final turn)
- **Pros:** Lightweight, fast, stable
- **Cons:** Shows tool calls in response, loses deep context
- **Best for:** General conversation, speed-critical use cases

**2. Qwen2.5-7B Q4 (4.4GB)**
- **Speed:** 3.26s average (nearly identical to Llama!)
- **Quality:** No hallucinations, very coherent
- **Memory:** Good but not perfect (same issue as Llama)
- **Pros:** Higher quality responses, better tool calling potential
- **Cons:** Larger model (2x size), shows tool calls in response
- **Best for:** Quality-critical conversations, tool calling

### üîç Context Loss Issue

**All models lost context** on the final turn of party planning test:
- User: "Remind me what we're planning?"
- Expected: "You're planning a birthday party for your friend Sarah next weekend"
- Actual: Tool calls, generic responses, or forgot Sarah entirely

**Root Cause:** Likely an issue with conversation history format or prompt, NOT the models themselves. The models receive conversation history but may not be properly guided to reference it.

### ‚ùå Not Recommended

**SmolLM2-1.7B**
- Generates hallucinations ("We are planning a picnic!" - nobody said picnic)
- Loses context frequently
- Too lightweight for complex conversations

**Qwen2.5-7B Q3**
- 2x slower than Q4 (7.56s vs 3.26s)
- No quality improvement over Q4
- Lower quantization causes slowdown without benefit

## Recommendations

### üèÜ Primary Recommendation: **Llama-3.2-3B**

**Why:**
1. ‚úÖ **Fastest** - 3.19s average (best speed-to-quality ratio)
2. ‚úÖ **Lightweight** - Only 1.9GB (good for Jetson)
3. ‚úÖ **Stable** - Zero hallucinations across all tests
4. ‚úÖ **Available** - Already downloaded and working
5. ‚úÖ **Proven** - Part of Meta's Llama 3.2 series (well-tested)

**Use Cases:**
- Default conversational model
- Voice assistant (speed critical)
- General chat interactions
- Dashboard widgets

### ü•à Secondary Recommendation: **Qwen2.5-7B Q4**

**Why:**
1. ‚úÖ **Best Tool Calling** - Qwen series has 90/100 score vs Llama's 35/100
2. ‚úÖ **Higher Quality** - More sophisticated responses
3. ‚úÖ **Nearly as fast** - Only 0.07s slower than Llama
4. ‚úÖ **Available** - Already downloaded

**Use Cases:**
- Action execution (calendar, lists, memory)
- Complex multi-step tasks
- When quality > speed
- Tool-heavy operations

### ‚öôÔ∏è Optimization Strategy: **Dual-Model System**

```python
# In model_config.py
def _get_best_conversation_model(self) -> str:
    """Fast, stable model for general chat"""
    return "llama3.2-3b"  # 3.19s, 75/100 quality

def _get_best_action_model(self) -> str:
    """High-quality model for tool calling"""
    return "qwen2.5-7b-q4"  # 3.26s, 90/100 tool score
```

This gives you:
- **Speed** for casual conversation (Llama)
- **Quality** for actions (Qwen)
- **Best of both worlds**

## Next Steps

### 1. Fix Context Loss Issue
The conversation history system needs investigation:
- Check how `conversation_history` is formatted in prompts
- Add explicit instruction to reference previous turns
- Test with explicit context in system prompt

### 2. Hide Tool Calls from Users
Both models expose `<tool_call>` in responses:
```
Response: "<tool_call>\n{\"name\": \"store_fact\"...}\n</tool_call>"
```

Fix in `chat.py`: Strip tool call markers before returning response.

### 3. Update Production Config

```yaml
# docker-compose.yml
zoe-llamacpp:
  environment:
    - MODEL_PATH=/models/llama-3.2-3b-gguf/Llama-3.2-3B-Instruct-Q4_K_M.gguf
    - MODEL_NAME=llama3.2-3b
```

```python
# model_config.py
def _get_best_conversation_model(self) -> str:
    return "phi3:mini"  # ‚Üê REPLACE THIS
    return "llama3.2-3b"  # ‚Üê WITH THIS
```

### 4. Monitor in Production
- Track response times
- Monitor hallucination reports from users
- A/B test Llama vs Qwen for 1 week
- Collect user feedback

## Conclusion

**Winner: Llama-3.2-3B** üèÜ

- Fastest: 3.19s average
- Stable: Zero hallucinations
- Lightweight: 1.9GB (Jetson-friendly)
- Proven: Battle-tested by Meta

**Deploy immediately** and monitor performance. Keep Qwen2.5-7B Q4 as backup for tool-heavy operations.

---

### Test Files Generated
- `results_llama3.2-3b_20251118_072141.json`
- `results_qwen2.5-7b-q3_20251118_072303.json`
- `results_qwen2.5-7b-q4_20251118_072410.json`
- `test_single_model.py` - Reusable test script
- `test_models_jetson.py` - Comprehensive testing framework

