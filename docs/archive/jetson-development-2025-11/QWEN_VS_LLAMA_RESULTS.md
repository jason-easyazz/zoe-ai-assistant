# üéØ Qwen 2.5 7B vs Llama 3.2 3B: ACTUAL BENCHMARK RESULTS

**Date:** 2025-11-13  
**Hardware:** Jetson Orin NX 16GB (MAXN_SUPER mode, clocks maximized)

---

## üìä PERFORMANCE COMPARISON

### Llama 3.2 3B (Q4_K_M) - Previously Tested
- **Generation Speed:** 27.17 tok/s ‚úÖ
- **Prompt Processing:** 48.02 tok/s
- **Model Size:** 1.9GB VRAM
- **Response Time (50 tokens):** ~1.8 seconds
- **GPU Offload:** 29/29 layers (100%)
- **Status:** FAST & RESPONSIVE for voice ‚úÖ

### Qwen 2.5 7B (Q3_K_M) - Current Test
- **Generation Speed:** 9.7 tok/s ‚ö†Ô∏è **SLOW**
- **Prompt Processing:** 29.6 tok/s
- **Model Size:** 3.4GB VRAM
- **Response Time (50 tokens):** ~5.2 seconds
- **GPU Offload:** 29/29 layers (100%)
- **Status:** **TOO SLOW for real-time voice** ‚ùå

---

## üö® CRITICAL FINDINGS

### Speed Deficit
| Metric | Llama 3.2 3B | Qwen 2.5 7B | Difference |
|--------|-------------|-------------|------------|
| **Tokens/sec** | 27.2 | 9.7 | **-64% (2.8x slower)** |
| **50 token response** | 1.8s | 5.2s | **+189% latency** |
| **100 token response** | 3.7s | 10.3s | **+178% latency** |

### Why So Slow?
**Expected:** 23.5 tok/s (per NVIDIA benchmarks)  
**Actual:** 9.7 tok/s  
**Gap:** 59% slower than expected!

**Possible reasons:**
1. **More parameters = slower inference** (7B vs 3B)
2. **Q3 quantization** may be slower than Q4 on this hardware
3. **Memory bandwidth bottleneck** (3.4GB vs 1.9GB)
4. **Different llama.cpp version** than NVIDIA's tests
5. **Competing processes** consuming resources
6. **Unified memory architecture** limiting throughput

---

## üéôÔ∏è REAL-TIME VOICE IMPACT

### Voice Conversation Requirements
- **Target latency:** < 2 seconds for natural conversation
- **Average response:** 30-50 tokens (one sentence)

### Latency Analysis

**Llama 3.2 3B (27 tok/s):**
```
30 tokens = 1.1s ‚úÖ Excellent
50 tokens = 1.8s ‚úÖ Great
100 tokens = 3.7s ‚úÖ Acceptable
```

**Qwen 2.5 7B (9.7 tok/s):**
```
30 tokens = 3.1s ‚ö†Ô∏è Noticeable lag
50 tokens = 5.2s ‚ùå Too slow
100 tokens = 10.3s ‚ùå Unusable
```

---

## ü§î INTELLIGENCE vs SPEED TRADE-OFF

### What You Get with Qwen 2.5 7B
‚úÖ **Better reasoning** (7B parameters vs 3B)  
‚úÖ **Better code generation**  
‚úÖ **Better instruction following**  
‚úÖ **Better tool calling (maybe)**  
‚úÖ **More context understanding**

### What You Lose
‚ùå **2.8x SLOWER generation** (9.7 vs 27 tok/s)  
‚ùå **Real-time voice becomes laggy** (5s responses)  
‚ùå **User experience degrades significantly**  
‚ùå **70% more VRAM** (3.4GB vs 1.9GB)  
‚ùå **Less headroom for other services** (Whisper, TTS)

---

## üéØ RECOMMENDATION

### For Real-Time Voice: **KEEP LLAMA 3.2 3B**

**Reasons:**
1. **Speed is CRITICAL for voice** - 1.8s feels natural, 5s feels broken
2. **Llama 3.2 3B is "smart enough"** - handles tool calling, natural language, memory
3. **More VRAM headroom** - can run Whisper + TTS simultaneously
4. **Proven stable** - no OOM errors, consistent performance
5. **Users value responsiveness over intelligence** in voice interfaces

### For Text/Complex Tasks: Consider Qwen via API

If you need Qwen-level intelligence for specific tasks:
- Use **external API** (OpenRouter, Groq, etc.) for complex reasoning
- Keep **Llama 3.2 3B on-device** for fast, real-time voice
- **Hybrid approach:** Fast local LLM + smart cloud LLM when needed

---

## üìà PERFORMANCE SUMMARY

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Model           ‚îÇ Speed        ‚îÇ Intelligence‚îÇ Voice Ready  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Llama 3.2 3B    ‚îÇ 27 tok/s ‚úÖ  ‚îÇ Good        ‚îÇ ‚úÖ YES       ‚îÇ
‚îÇ Qwen 2.5 7B Q3  ‚îÇ 9.7 tok/s ‚ö†Ô∏è‚îÇ Excellent   ‚îÇ ‚ùå TOO SLOW  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚úÖ FINAL DECISION

**Switch BACK to Llama 3.2 3B for production voice use.**

Qwen 2.5 7B is impressive, but **speed matters more than intelligence** for real-time voice conversations. The 2.8x slower performance makes it unsuitable for your use case.

---

## üîÑ ACTION ITEMS

1. ‚úÖ Benchmark complete
2. ‚è≠Ô∏è Switch back to Llama 3.2 3B
3. ‚è≠Ô∏è Update docker-compose.yml
4. ‚è≠Ô∏è Test voice latency end-to-end
5. ‚è≠Ô∏è Archive Qwen as "tested but too slow"

**Status:** Analysis complete, ready to revert to Llama 3.2 3B.





