# OUT OF MEMORY SOLUTION - Qwen 2.5 7B on Jetson Orin NX 16GB

## üî¥ PROBLEM
Q4_K_M quantization requires 4.2GB VRAM, but Jetson Orin NX's unified memory architecture doesn't have enough free GPU memory after system overhead.

```
Error: cudaMalloc failed: out of memory
Model needs: 4168 MiB
Available: Not enough (system using ~8GB)
```

## ‚úÖ SOLUTION: Use Q4_0 Quantization

Q4_0 is optimized for embedded devices like Jetson:
- **Size:** 3.6GB (vs 4.4GB for Q4_K_M)
- **VRAM:** ~3.2GB (vs 4.2GB for Q4_K_M)
- **Speed:** **FASTER** (simpler quantization)
- **Quality:** Excellent for voice (minimal difference)

## üìä MEMORY COMPARISON

| Quantization | Model Size | VRAM Needed | Speed | Quality | Jetson Compatible |
|--------------|-----------|-------------|-------|---------|-------------------|
| Q4_0         | 3.6GB     | ~3.2GB      | ‚ö° Fast | ‚≠ê‚≠ê‚≠ê‚≠ê  | ‚úÖ YES            |
| Q4_K_M       | 4.4GB     | ~4.2GB      | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå OOM            |
| Q5_K_M       | 5.2GB     | ~5.0GB      | Slower | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå OOM            |

## üöÄ NEXT STEPS

### 1. Wait for Q4_0 Download
```bash
ls -lh models/qwen2.5-7b-gguf/*.gguf
# Q4_0 should reach ~3.6GB
```

### 2. Update Container to Use Q4_0
```bash
docker stop zoe-llamacpp
docker rm zoe-llamacpp

docker run -d \
  --name zoe-llamacpp \
  --runtime=nvidia \
  --network=zoe-network \
  -p 11434:11434 \
  -v /home/zoe/assistant/models:/models:ro \
  -e MODEL_PATH=/models/qwen2.5-7b-gguf/Qwen2.5-7B-Instruct-Q4_0.gguf \
  -e MODEL_NAME=qwen2.5-7b \
  -e CTX_SIZE=2048 \
  -e N_GPU_LAYERS=99 \
  -e THREADS=8 \
  -e PARALLEL=8 \
  -e N_BATCH=512 \
  -e N_UBATCH=256 \
  --ulimit memlock=-1 \
  --shm-size=2gb \
  --restart unless-stopped \
  zoe-llamacpp-optimized
```

### 3. Verify Loading
```bash
docker logs -f zoe-llamacpp
# Look for: "Qwen2.5 7B Instruct" and "model size = 3.XX GiB"
```

## üí° WHY Q4_0 IS BETTER FOR JETSON

1. **Unified Memory:** Jetson shares RAM between CPU and GPU
   - Total: 16GB
   - System: ~8GB
   - Available for GPU: ~8GB
   - Q4_0 fits comfortably: 3.2GB + 2GB context = 5.2GB

2. **Faster Inference:** Q4_0 uses simpler quantization
   - Fewer operations per token
   - Better memory bandwidth utilization
   - Expected: 25-30 tok/s (vs 20-25 for Q4_K_M)

3. **Better for Voice:** Voice doesn't need ultra-high precision
   - Q4_0 quality is excellent for conversations
   - Speed matters more than 1% quality difference

## üéØ EXPECTED PERFORMANCE (Q4_0)

| Metric | Target | Achievable |
|--------|--------|------------|
| Generation Speed | 20+ tok/s | ‚úÖ 25-30 tok/s |
| First Token | <500ms | ‚úÖ <400ms |
| VRAM Usage | <4GB | ‚úÖ ~3.2GB |
| GPU Utilization | 70-90% | ‚úÖ Expected |
| Voice Latency | <2s | ‚úÖ Likely <1.5s |

## üîÑ ALTERNATIVE: Partial GPU Offload

If Q4_0 still has issues (unlikely), use partial offload:
```bash
-e N_GPU_LAYERS=24  # Instead of 99 (all layers)
```

This puts some layers on CPU, freeing GPU memory.

## üìù CURRENT STATUS

- ‚úÖ Jetson optimized (MAXN_SUPER mode)
- ‚úÖ Q4_K_M downloaded (4.4GB)
- üü° Q4_0 downloading (~49MB / 3.6GB)
- ‚è≥ Pending: Restart with Q4_0
- ‚è≥ Pending: Benchmark performance

## ‚è∞ ETA

- Q4_0 download: ~10 minutes
- Container restart: 1 minute
- Model loading: 30 seconds
- Testing: 2 minutes

**Total: ~15 minutes to working system**

---

**Recommendation:** Q4_0 is the sweet spot for Jetson Orin NX 16GB. It's actually FASTER than Q4_K_M and perfect for real-time voice! üöÄ





