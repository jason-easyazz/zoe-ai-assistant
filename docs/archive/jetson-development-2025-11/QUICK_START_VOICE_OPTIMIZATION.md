# ðŸš€ QUICK START: Real-Time Voice Optimization

**Goal:** Qwen 2.5 7B with 20+ tok/s for natural conversations  
**Current:** Downloading model (171MB / 4.4GB)  
**ETA:** 15-20 minutes

---

## ðŸ“Š CURRENT STATUS

âœ… **Optimization Configuration Created**
âœ… **Docker Config Updated** (Qwen 2.5 7B with optimized settings)
âœ… **Jetson Scripts Created** (performance optimization)
ðŸŸ¡ **Downloading Model** (4.4GB, in progress)
â³ **Pending:** Service restart + benchmark

---

## ðŸŽ¯ WHAT'S BEEN OPTIMIZED

### Model Upgrade
- **From:** Llama 3.2 3B (~2GB)
- **To:** Qwen 2.5 7B Q4_K_M (~4.4GB)
- **Why:** Better intelligence, more natural conversations, superior tool calling

### Performance Settings
```yaml
Context Size:   2048  (was 4096) â†’ 2x faster
CPU Threads:    8     (was 6)    â†’ More parallelism
Parallel Seqs:  8     (was 4)    â†’ Concurrent requests
Batch Size:     512   (was none) â†’ Faster prompt processing
Micro-batch:    256   (was none) â†’ Faster generation
```

### Advanced Flags
- `--cont-batching` - Continuous batching for multi-user
- `--flash-attn` - Flash attention for speed
- `--mlock` - Lock model in RAM (no swapping)
- Async CUDA enabled

---

## ðŸ”§ NEXT STEPS (DO THESE IN ORDER)

### Step 1: Monitor Download (NOW)
```bash
# Watch download progress
watch -n 5 "ls -lh models/qwen2.5-7b-gguf/*.gguf 2>&1 | tail -1"

# When it reaches ~4.4GB, proceed to Step 2
```

**Expected:** File will grow from 171MB â†’ 4.4GB (~10-15 minutes)

---

### Step 2: Optimize Jetson Hardware (REQUIRES SUDO)
```bash
# Set Jetson to MAXIMUM performance mode
sudo bash scripts/setup/optimize_jetson_performance.sh
```

**This will:**
- Set power mode to MAXN (maximum performance)
- Maximize all clock speeds (CPU, GPU, memory)
- Verify GPU frequency

**IMPORTANT:** Run this BEFORE restarting services for best performance!

---

### Step 3: Restart llama.cpp Service
```bash
# Restart with new model and optimized settings
docker-compose restart zoe-llamacpp

# Watch startup (should load Qwen 2.5 7B)
docker-compose logs -f zoe-llamacpp
```

**Look for:**
```
ðŸš€ Starting llama.cpp Server (OPTIMIZED FOR REAL-TIME VOICE)
Model: /models/qwen2.5-7b-gguf/Qwen2.5-7B-Instruct-Q4_K_M.gguf
Context: 2048 tokens (optimized for voice)
GPU Layers: 99
```

**Press Ctrl+C to exit logs when you see "listening on http://0.0.0.0:11434"**

---

### Step 4: Quick Test
```bash
# Simple test (should respond in <2s)
curl -X POST http://localhost:11434/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen2.5-7b","prompt":"Hi Zoe, how are you?","max_tokens":50}'
```

**Expected:** Fast response with natural language

---

### Step 5: Benchmark Performance
```bash
# Run benchmark suite (measures tokens/sec, latency, GPU usage)
# TODO: Create benchmark script
```

**Target Metrics:**
- Generation speed: 20+ tok/s
- First token: <500ms
- GPU utilization: 70-90%

---

### Step 6: Test Voice Pipeline
```bash
# Test full voice flow: STT â†’ LLM â†’ TTS
# Use Zoe UI voice interface
# Target: <2s total response time
```

---

## ðŸ“Š MONITORING COMMANDS

### GPU Utilization
```bash
# Real-time GPU monitoring
watch -n 0.5 nvidia-smi

# Jetson-specific stats
tegrastats --interval 500
```

**Target:** 70-90% GPU utilization during inference

### Memory Usage
```bash
# Docker container memory
docker stats zoe-llamacpp

# System memory
free -h
```

**Expected:** Qwen 2.5 7B uses ~5-6GB RAM

---

## âš ï¸ TROUBLESHOOTING

### Problem: Model download slow
**Solution:** Wait patiently (4.4GB on Jetson network)
**Alternative:** Download on faster machine, transfer via SCP

### Problem: Out of memory on startup
**Solution:**
1. Check swap is enabled: `swapon --show`
2. Reduce context to 1024: Edit docker-compose.yml `CTX_SIZE=1024`
3. Try Q4_0 quantization (smaller, faster)

### Problem: Still slow (<10 tok/s)
**Solutions:**
1. Verify Jetson optimization ran: `sudo jetson_clocks --show`
2. Check GPU layers loaded: Look for "99" in logs
3. Monitor GPU utilization: Should be 70-90%
4. Try reducing parallel sequences to 4

### Problem: Service won't start
**Solution:**
1. Check model file complete: `ls -lh models/qwen2.5-7b-gguf/*.gguf`
2. Should be ~4.4GB, if smaller, re-download
3. Check logs: `docker-compose logs zoe-llamacpp | tail -50`

---

## ðŸŽ¯ SUCCESS CRITERIA

âœ… **Qwen 2.5 7B loads successfully**  
âœ… **Generation speed: 20+ tok/s**  
âœ… **First token latency: <500ms**  
âœ… **GPU utilization: 70-90%**  
âœ… **Voice response: <2s total (STTâ†’LLMâ†’TTS)**  
âœ… **Natural, fluid conversations**

---

## ðŸ“ KEY FILES

| File | Purpose |
|------|---------|
| `LLAMACPP_OPTIMIZATION_PLAN.md` | Detailed optimization strategy |
| `REALTIME_VOICE_STATUS.md` | Current status and metrics |
| `scripts/setup/optimize_jetson_performance.sh` | Jetson hardware optimization |
| `scripts/setup/download_qwen_optimized.sh` | Model download script |
| `services/zoe-llamacpp/entrypoint-optimized.sh` | Optimized startup script |
| `docker-compose.yml` | Updated with Qwen + optimizations |

---

## ðŸ’¡ WHY THESE CHANGES?

**Qwen 2.5 7B:**
- 2-3x more intelligent than Llama 3.2 3B
- Better instruction following
- More natural conversational style
- Superior code generation
- Industry-leading tool calling

**Aggressive Optimization:**
- Reduced context (2048 vs 4096): Voice doesn't need long context
- Increased parallelism (8 vs 4): Handle concurrent requests
- Larger batches (512): Faster prompt processing
- Flash attention: State-of-the-art speed optimization
- Async CUDA: Remove blocking overhead

**Expected Result:**
5 tok/s â†’ 20+ tok/s (4x improvement) = Real-time voice conversations!

---

**Current Status:** ðŸŸ¡ Downloading model (171MB / 4.4GB)  
**Next Action:** Wait for download, then run Step 2 (Jetson optimization with sudo)  
**ETA to Production:** 20-25 minutes

ðŸš€ **Let's make Zoe FAST!**





