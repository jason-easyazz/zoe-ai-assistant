# Integration: Experts + LLMs + MCP Server

**Date**: October 8, 2025  
**Question**: "Does this play nicely with LLMs and MCP server?"  
**Answer**: **YES! They work together beautifully!**

---

## ðŸŽ¯ How The System Works Together

### 1. User Makes Request (Natural Language)
```
User: "Remember a person named John and turn on the lights"
```

### 2. Chat Router Receives Request
**File**: `services/zoe-core/routers/chat.py`

```python
# Step 1: Try EnhancedMemAgent first (for actions)
memories = await enhanced_mem_agent.enhanced_search(
    message, 
    user_id=user_id,
    execute_actions=True
)

# If actions executed, return expert result
if memories.get("actions_executed", 0) > 0:
    return expert_response

# Step 2: Fall back to LLM conversation
response = await call_ollama_with_context(message, context, memories)
```

### 3. EnhancedMemAgent Routes to Experts
**Service**: `mem-agent` (port 11435)

```python
# Each expert has confidence score
person_confidence = PersonExpert.can_handle(query)  # 0.95 for "person named"
ha_confidence = HomeAssistantExpert.can_handle(query)  # 0.95 for "turn on"

# Execute all experts with confidence > 0.5
if person_confidence > 0.5:
    person_result = await PersonExpert.execute(query, user_id)
    
if ha_confidence > 0.5:
    ha_result = await HomeAssistantExpert.execute(query, user_id)
```

### 4. Experts Call APIs or MCP Tools
**PersonExpert** â†’ Calls `/api/memories/` directly  
**HomeAssistantExpert** â†’ Calls `/api/homeassistant/` OR delegates to MCP  
**ListExpert** â†’ Calls `/api/lists/` directly  
**JournalExpert** â†’ Calls `/api/journal/` directly

### 5. MCP Server Provides Additional Tools
**Service**: `zoe-mcp-server` (port 8003)

**15+ Tools Available**:
- `create_person`, `search_people`
- `create_event`, `search_events`
- `add_to_list`, `get_list`
- `set_reminder`
- `control_device` (Home Assistant)
- `run_workflow` (N8N)

### 6. LLM Generates Response
**RouteLLM** chooses model:
- Simple queries â†’ Ollama (local, fast)
- Complex queries â†’ Claude/GPT-4 (powerful)

**LLM receives**:
- Original query
- Expert execution results
- User context (calendar, lists, memories)
- MCP tool results
- Temporal memory (previous conversations)

**LLM generates**: Natural, conversational response

---

## ðŸ”„ Complete Flow Example

### Example: "Remember John Smith and remind me to call him tomorrow"

```
1. User Request
   â†“
2. Chat Router
   â†“
3. EnhancedMemAgent (port 11435)
   â”œâ”€â†’ PersonExpert (confidence: 0.95)
   â”‚   â””â”€â†’ POST /api/memories/ (creates John Smith)
   â”‚       âœ… "I'll remember John Smith"
   â”‚
   â””â”€â†’ ReminderExpert (confidence: 0.95)
       â””â”€â†’ POST /api/reminders/ (creates reminder)
           âœ… "I'll remind you tomorrow"
   â†“
4. Actions Executed: 2
   â†“
5. Return: "âœ… I'll remember John Smith and remind you to call him tomorrow"
```

**Result**: Natural language â†’ 2 API calls â†’ Success message

**NO LLM needed** for simple actions! (Faster, cheaper)

---

## ðŸ§  When Each Component Is Used

### Experts (mem-agent) - FOR ACTIONS
**Use When**: User wants to DO something  
**Examples**:
- "Add bread to list" â†’ ListExpert executes
- "Create event tomorrow" â†’ CalendarExpert executes
- "Remember person X" â†’ PersonExpert executes
- "Turn on lights" â†’ HomeAssistantExpert executes

**Benefit**: Fast, direct, no LLM needed

---

### LLM (RouteLLM + Ollama/Claude) - FOR CONVERSATION
**Use When**: User wants to TALK or query requires intelligence  
**Examples**:
- "How are you?" â†’ LLM conversation
- "What should I do today?" â†’ LLM + context
- "Tell me about my week" â†’ LLM + memories
- "Give me advice about X" â†’ LLM intelligence

**Benefit**: Natural, intelligent, conversational

---

### MCP Server - FOR TOOLS
**Use When**: Experts need structured tool calls OR LLM needs tools  
**Examples**:
- Expert delegates: `{"mcp_delegate": True}`
- LLM tool calls: `<tool>create_person</tool>`
- Direct API: `POST /api/mcp/tools/execute`

**Benefit**: Standardized tool interface, structured execution

---

## ðŸ“Š Current Expert Integration

### 9 Experts Active

| Expert | Purpose | Integrates With |
|--------|---------|-----------------|
| PersonExpert | People/relationships | API + MCP |
| ListExpert | Shopping/tasks | API + MCP |
| CalendarExpert | Events/scheduling | API + MCP |
| MemoryExpert | Notes/projects | API + LightRAG |
| PlanningExpert | Goal planning | API + AgentPlanner |
| JournalExpert | Journal entries | API |
| ReminderExpert | Reminders | API |
| HomeAssistantExpert | Smart home | API + MCP + HA |
| BirthdayExpert | Birthdays/gifts | API |

**All experts CAN use MCP tools as needed!**

---

## ðŸŽ¯ Integration Pattern

### Pattern 1: Expert â†’ Direct API (Fast)
```python
# PersonExpert creates person
response = await client.post(
    f"{api_base}/memories/",
    headers={"X-Service-Token": token},
    json=person_data
)
# Direct, fast, no LLM overhead
```

### Pattern 2: Expert â†’ MCP Tool (Structured)
```python
# HomeAssistantExpert uses MCP
return {
    "mcp_delegate": True,
    "tool": "control_device",
    "params": {"device": "lights", "action": "turn_on"}
}
# Let MCP handle structured execution
```

### Pattern 3: LLM â†’ Expert â†’ Action (Intelligent)
```python
# LLM understands intent
intent = await llm.analyze(message)

# Route to expert
if intent == "create_person":
    result = await PersonExpert.execute(message, user_id)
    
# LLM generates natural response
response = await llm.generate(result, context)
```

---

## âœ… ANSWER TO YOUR QUESTION

**Q**: "Does this play nicely with LLMs and MCP server?"

**A**: **PERFECTLY!** Here's how:

### LLMs Handle:
- âœ… Intent understanding
- âœ… Natural language generation
- âœ… Conversational intelligence
- âœ… Context-aware responses
- âœ… Routing decisions (via RouteLLM)

### Experts Handle:
- âœ… Action execution
- âœ… API calls
- âœ… Data extraction from NL
- âœ… Fast response for simple actions
- âœ… Specialized domain logic

### MCP Server Provides:
- âœ… Tool registry (15+ tools)
- âœ… Structured execution
- âœ… Service integration (HA, N8N)
- âœ… Fallback when experts delegate

### They Work Together:
1. **Fast Path**: NL â†’ Expert â†’ API â†’ Done (no LLM)
2. **Smart Path**: NL â†’ Expert â†’ API + LLM response
3. **Complex Path**: NL â†’ LLM â†’ Multiple Experts â†’ MCP Tools â†’ LLM synthesis

---

## ðŸŽ‰ Result

**9 Experts** + **RouteLLM** + **MCP Server** = Intelligent, Fast, Flexible AI

- Simple actions: Use experts (fast)
- Complex queries: Use LLM (intelligent)
- Structured tools: Use MCP (reliable)
- **All work together seamlessly!** âœ¨

---

*Last Updated: October 8, 2025*  
*System: 9 Expert Multi-Agent System*

