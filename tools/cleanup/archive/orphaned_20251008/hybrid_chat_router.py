"""
Hybrid Chat Router for Zoe - Solution 3
=======================================

Always provides full conversational AI responses with smart enhancement system integration.
"""

from fastapi import APIRouter, Query
from pydantic import BaseModel
from typing import Optional, Dict, Any
import requests
import json
import time
import uuid
import logging
from datetime import datetime

router = APIRouter(tags=["chat"])
logger = logging.getLogger(__name__)

class ChatMessage(BaseModel):
    message: str
    context: Optional[Dict[str, Any]] = None

def analyze_question_type(message: str) -> Dict[str, bool]:
    """Analyze what enhancement systems are needed"""
    message_lower = message.lower()
    
    return {
        "needs_temporal_memory": any(word in message_lower for word in 
            ["remember", "earlier", "previous", "yesterday", "last time", "before", "history", "discussed"]),
        
        "needs_orchestration": len([word for word in message_lower.split() if word in 
            ["schedule", "create", "add", "plan", "organize", "coordinate", "help", "need"]]) >= 2,
        
        "needs_satisfaction_tracking": any(word in message_lower for word in 
            ["feedback", "satisfaction", "adapt", "learn", "improve", "feeling", "how am i"]),
        
        "is_enhancement_query": any(word in message_lower for word in 
            ["enhancement", "system", "capability", "feature", "temporal", "collaboration", "memory"])
    }

async def get_full_ai_response(message: str, user_id: str, context: Dict[str, Any]) -> str:
    """Always generate full conversational AI response"""
    try:
        # Create enhancement-aware prompt
        prompt = f"""You are Zoe, an advanced AI assistant with powerful enhancement systems:

üß† TEMPORAL MEMORY: You remember conversations across time and can reference previous discussions
ü§ù CROSS-AGENT COLLABORATION: You coordinate with 7 expert systems (Calendar, Lists, Memory, Planning, Development, Weather, HomeAssistant)
üòä USER SATISFACTION: You track user feedback and continuously adapt to improve your responses  
üöÄ CONTEXT CACHING: You optimize performance with intelligent caching for faster responses

User ({user_id}) is asking: "{message}"

Please respond naturally, conversationally, and helpfully. Be engaging and show your personality. When relevant, mention how your enhancement systems help you provide better assistance."""

        # Try Ollama first (best quality)
        try:
            ollama_response = requests.post("http://zoe-ollama:11434/api/generate",
                json={
                    "model": "gemma3:1b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.8,
                        "top_p": 0.9,
                        "max_tokens": 400
                    }
                },
                timeout=15
            )
            
            if ollama_response.status_code == 200:
                ollama_data = ollama_response.json()
                response_text = ollama_data.get('response', '').strip()
                
                if len(response_text) > 50:  # Good response
                    logger.info(f"‚úÖ Full AI response generated ({len(response_text)} chars)")
                    return response_text
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ollama unavailable: {e}")
        
        # Fallback to LiteLLM
        try:
            litellm_response = requests.post("http://zoe-litellm:8001/chat/completions",
                json={
                    "model": "gemma3-ultra-fast",
                    "messages": [
                        {"role": "system", "content": "You are Zoe, an AI assistant with enhancement systems."},
                        {"role": "user", "content": message}
                    ],
                    "max_tokens": 400,
                    "temperature": 0.8
                },
                timeout=10
            )
            
            if litellm_response.status_code == 200:
                litellm_data = litellm_response.json()
                response_text = litellm_data.get('choices', [{}])[0].get('message', {}).get('content', '').strip()
                
                if len(response_text) > 50:
                    logger.info(f"‚úÖ LiteLLM response generated ({len(response_text)} chars)")
                    return response_text
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LiteLLM unavailable: {e}")
        
        # Final fallback - generate helpful response
        return f"I'm here to help you with your request: '{message}'. My enhancement systems including temporal memory, cross-agent collaboration, and satisfaction tracking are all working to provide you with the best assistance possible!"
        
    except Exception as e:
        logger.error(f"‚ùå AI response generation failed: {e}")
        return f"I apologize, but I encountered a technical issue. However, my enhancement systems are still working to help you: {str(e)}"

async def integrate_enhancement_systems(ai_response: str, message: str, user_id: str) -> str:
    """Intelligently integrate enhancement systems based on content"""
    try:
        question_analysis = analyze_question_type(message)
        enhanced_response = ai_response
        
        # Temporal Memory Integration
        if question_analysis["needs_temporal_memory"]:
            try:
                # Get or create active episode
                episode_response = requests.get(f"http://localhost:8000/api/temporal-memory/episodes/active?user_id={user_id}", timeout=3)
                if episode_response.status_code == 200:
                    episode_data = episode_response.json()
                    episode = episode_data.get("episode")
                    
                    if not episode:
                        # Create new episode
                        create_response = requests.post("http://localhost:8000/api/temporal-memory/episodes",
                            json={"context_type": "chat", "participants": [user_id]},
                            params={"user_id": user_id},
                            timeout=3
                        )
                        if create_response.status_code == 200:
                            episode = create_response.json()["episode"]
                    
                    if episode:
                        # Record conversation in episode
                        requests.post(f"http://localhost:8000/api/temporal-memory/episodes/{episode['id']}/messages",
                            params={"message": message, "message_type": "user", "user_id": user_id},
                            timeout=2
                        )
                        requests.post(f"http://localhost:8000/api/temporal-memory/episodes/{episode['id']}/messages",
                            params={"message": enhanced_response, "message_type": "assistant", "user_id": user_id},
                            timeout=2
                        )
                        
                        enhanced_response += f"\n\nüí≠ *I've recorded this in our conversation episode for future reference.*"
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Temporal memory integration failed: {e}")
        
        # Cross-Agent Orchestration
        if question_analysis["needs_orchestration"]:
            try:
                orchestration_response = requests.post("http://localhost:8000/api/orchestration/orchestrate",
                    json={"request": message, "context": {"chat_integration": True}},
                    params={"user_id": user_id},
                    timeout=10
                )
                
                if orchestration_response.status_code == 200:
                    orchestration_data = orchestration_response.json()
                    if orchestration_data.get("success"):
                        expert_count = len(orchestration_data.get("decomposed_tasks", []))
                        enhanced_response += f"\n\nü§ù *I've coordinated with {expert_count} expert systems to handle your request.*"
                    else:
                        enhanced_response += "\n\nü§ù *I attempted to coordinate multiple experts, but some tasks encountered issues.*"
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Orchestration integration failed: {e}")
        
        # Enhancement System Awareness
        if question_analysis["is_enhancement_query"]:
            enhanced_response += "\n\nüåü *My enhancement systems (temporal memory, cross-agent collaboration, user satisfaction tracking, and context caching) are all working together to provide you with the best possible assistance!*"
        
        return enhanced_response
        
    except Exception as e:
        logger.error(f"‚ùå Enhancement integration failed: {e}")
        return ai_response  # Return original response if enhancement fails

async def record_for_learning(message: str, response: str, user_id: str, response_time: float):
    """Record interaction for satisfaction tracking and learning"""
    try:
        interaction_id = str(uuid.uuid4())
        
        # Record interaction for satisfaction analysis
        requests.post("http://localhost:8000/api/satisfaction/interaction",
            json={
                "interaction_id": interaction_id,
                "request_text": message,
                "response_text": response,
                "response_time": response_time,
                "context": {"hybrid_chat": True, "enhancement_systems_active": True}
            },
            params={"user_id": user_id},
            timeout=3
        )
        
        logger.info(f"‚úÖ Interaction recorded for learning: {interaction_id}")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Learning integration failed: {e}")

@router.post("/api/chat/")
@router.post("/api/chat")
async def hybrid_chat(msg: ChatMessage, user_id: str = Query("default", description="User ID for privacy isolation")):
    """Hybrid chat with always full AI + smart enhancement integration"""
    start_time = time.time()
    
    try:
        logger.info(f"üó£Ô∏è Hybrid chat request from {user_id}: {msg.message[:50]}...")
        
        # Layer 1: Always get full conversational AI response
        ai_response = await get_full_ai_response(msg.message, user_id, msg.context or {})
        
        # Layer 2: Smart enhancement system integration
        enhanced_response = await integrate_enhancement_systems(ai_response, msg.message, user_id)
        
        # Layer 3: Record for learning and adaptation
        response_time = time.time() - start_time
        await record_for_learning(msg.message, enhanced_response, user_id, response_time)
        
        logger.info(f"‚úÖ Hybrid chat completed ({response_time:.2f}s, {len(enhanced_response)} chars)")
        
        return {
            "response": enhanced_response,
            "response_time": response_time,
            "enhancement_systems_used": analyze_question_type(msg.message),
            "hybrid_chat_active": True
        }
        
    except Exception as e:
        response_time = time.time() - start_time
        error_response = f"I apologize, but I encountered a technical issue. My enhancement systems are still working to help you though! Error: {str(e)}"
        
        logger.error(f"‚ùå Hybrid chat error: {e}")
        
        return {
            "response": error_response,
            "response_time": response_time,
            "error": str(e),
            "hybrid_chat_active": False
        }


