#!/bin/bash
# COMPLETE_ROUTELLM_ACTIVATION.sh
# Location: scripts/development/complete_routellm_activation.sh
# Purpose: Complete script to activate RouteLLM with all providers

set -e

echo "üöÄ COMPLETE ROUTELLM ACTIVATION"
echo "================================"
echo ""
echo "This script will:"
echo "  1. Ensure API keys are loaded in containers"
echo "  2. Discover all available models dynamically"
echo "  3. Enable intelligent routing"
echo "  4. Test everything works"
echo ""
echo "Press Enter to continue..."
read

cd /home/pi/zoe

# Step 1: Check current state
echo -e "\nüìä Current State Check:"
echo "======================="

echo -e "\n1Ô∏è‚É£ Checking API keys in .env..."
if [ -f .env ]; then
    for key in OPENAI_API_KEY ANTHROPIC_API_KEY GOOGLE_API_KEY MISTRAL_API_KEY; do
        if grep -q "$key=" .env; then
            echo "  ‚úÖ $key found"
        else
            echo "  ‚ö™ $key not configured"
        fi
    done
else
    echo "  ‚ùå No .env file!"
fi

echo -e "\n2Ô∏è‚É£ Checking docker-compose.yml..."
if grep -A 10 "zoe-core:" docker-compose.yml | grep -q "env_file:"; then
    echo "  ‚úÖ env_file directive present"
else
    echo "  ‚ö†Ô∏è env_file directive missing - will add"
fi

# Step 2: Fix docker-compose if needed
echo -e "\nüîß Ensuring Docker Configuration..."
if ! grep -A 10 "zoe-core:" docker-compose.yml | grep -q "env_file:"; then
    echo "Adding env_file to docker-compose.yml..."
    
    # Backup
    cp docker-compose.yml docker-compose.yml.backup_$(date +%Y%m%d_%H%M%S)
    
    # Add env_file directive using sed
    sed -i '/zoe-core:/,/^  [^ ]/ {
        /container_name: zoe-core/a\    env_file: .env
    }' docker-compose.yml
    
    echo "‚úÖ docker-compose.yml updated"
    
    # Restart container to load environment
    echo "Restarting container to load environment..."
    docker compose down zoe-core
    docker compose up -d zoe-core
    sleep 10
fi

# Step 3: Verify keys in container
echo -e "\nüîë Verifying API Keys in Container..."
docker exec zoe-core python3 << 'VERIFY_KEYS'
import os
import sys

keys_found = False

print("API Key Status:")
print("-" * 40)

for provider, env_var in [
    ("OpenAI", "OPENAI_API_KEY"),
    ("Anthropic", "ANTHROPIC_API_KEY"),
    ("Google", "GOOGLE_API_KEY"),
    ("Mistral", "MISTRAL_API_KEY"),
    ("Cohere", "COHERE_API_KEY"),
    ("Groq", "GROQ_API_KEY")
]:
    key = os.getenv(env_var, "")
    if key and key not in ["", "your-key-here", "your-api-key-here"]:
        print(f"‚úÖ {provider}: {key[:10]}...")
        if provider in ["OpenAI", "Anthropic"]:
            keys_found = True
    else:
        print(f"‚ö™ {provider}: Not configured")

if not keys_found:
    print("\n‚ùå No valid API keys found!")
    print("Please check your .env file")
    sys.exit(1)
else:
    print("\n‚úÖ API keys loaded successfully")
VERIFY_KEYS

# Step 4: Run complete model discovery
echo -e "\nüîç Running Complete Model Discovery..."
docker exec zoe-core python3 << 'DISCOVER_ALL'
import os
import json
import httpx
import asyncio
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def discover_all_models():
    config_file = "/app/data/llm_models.json"
    
    # Load existing config
    with open(config_file) as f:
        config = json.load(f)
    
    print("\nüîç DISCOVERING MODELS FROM ALL PROVIDERS")
    print("=" * 50)
    
    discovered_any = False
    
    # 1. OpenAI Discovery
    openai_key = os.getenv("OPENAI_API_KEY", "").strip()
    if openai_key and not openai_key.startswith("your"):
        print("\nüì° OpenAI:")
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    "https://api.openai.com/v1/models",
                    headers={"Authorization": f"Bearer {openai_key}"},
                    timeout=15
                )
                
                if response.status_code == 200:
                    data = response.json()
                    all_models = [m["id"] for m in data["data"]]
                    
                    # Filter and prioritize GPT models
                    gpt_models = sorted([m for m in all_models if "gpt" in m.lower()])
                    
                    # Select best models
                    selected = []
                    for priority in ["gpt-4-turbo", "gpt-4o", "gpt-4", "gpt-3.5-turbo"]:
                        matches = [m for m in gpt_models if priority in m]
                        if matches:
                            selected.extend(matches[:2])
                    
                    # Add any other GPT models
                    for model in gpt_models:
                        if model not in selected and len(selected) < 10:
                            selected.append(model)
                    
                    if selected:
                        config["providers"]["openai"]["enabled"] = True
                        config["providers"]["openai"]["models"] = selected
                        config["providers"]["openai"]["default"] = selected[0]
                        print(f"  ‚úÖ Found {len(selected)} models")
                        for model in selected[:3]:
                            print(f"     - {model}")
                        if len(selected) > 3:
                            print(f"     ... and {len(selected)-3} more")
                        discovered_any = True
                else:
                    print(f"  ‚ùå API returned status {response.status_code}")
                    
        except Exception as e:
            print(f"  ‚ùå Error: {str(e)[:100]}")
    
    # 2. Anthropic Discovery
    anthropic_key = os.getenv("ANTHROPIC_API_KEY", "").strip()
    if anthropic_key and not anthropic_key.startswith("your"):
        print("\nüì° Anthropic:")
        
        # Test known Claude models
        claude_models = [
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229",
            "claude-3-haiku-20240307",
            "claude-2.1",
            "claude-instant-1.2"
        ]
        
        working_models = []
        
        try:
            async with httpx.AsyncClient() as client:
                for model in claude_models:
                    try:
                        response = await client.post(
                            "https://api.anthropic.com/v1/messages",
                            headers={
                                "x-api-key": anthropic_key,
                                "anthropic-version": "2023-06-01"
                            },
                            json={
                                "model": model,
                                "max_tokens": 1,
                                "messages": [{"role": "user", "content": "test"}]
                            },
                            timeout=10
                        )
                        
                        if response.status_code in [200, 201]:
                            working_models.append(model)
                            print(f"  ‚úÖ {model}")
                        elif response.status_code != 404:  # Model might exist but request failed
                            working_models.append(model)
                            print(f"  ‚úÖ {model} (available)")
                            
                    except Exception:
                        pass
                
                if working_models:
                    config["providers"]["anthropic"]["enabled"] = True
                    config["providers"]["anthropic"]["models"] = working_models
                    config["providers"]["anthropic"]["default"] = working_models[0]
                    print(f"  Total: {len(working_models)} models available")
                    discovered_any = True
                    
        except Exception as e:
            print(f"  ‚ùå Error: {str(e)[:100]}")
    
    # 3. Ollama Discovery (Local)
    print("\nüì° Ollama (Local):")
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                "http://zoe-ollama:11434/api/tags",
                timeout=5
            )
            
            if response.status_code == 200:
                data = response.json()
                models = [m["name"] for m in data.get("models", [])]
                
                if models:
                    config["providers"]["ollama"]["enabled"] = True
                    config["providers"]["ollama"]["models"] = models
                    config["providers"]["ollama"]["default"] = models[0]
                    print(f"  ‚úÖ Found {len(models)} models: {models}")
                    discovered_any = True
                    
    except Exception as e:
        print(f"  ‚ùå Error: {str(e)[:50]}")
    
    # 4. Update configuration
    if discovered_any:
        # Set intelligent default provider
        if config["providers"]["anthropic"]["enabled"]:
            config["default_provider"] = "anthropic"
        elif config["providers"]["openai"]["enabled"]:
            config["default_provider"] = "openai"
        else:
            config["default_provider"] = "ollama"
        
        config["last_updated"] = datetime.now().isoformat()
        
        # Save configuration
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        
        print("\n" + "=" * 50)
        print("‚úÖ DISCOVERY COMPLETE!")
        print(f"Default Provider: {config['default_provider']}")
        
        # Summary
        total_models = 0
        for provider, data in config["providers"].items():
            if data["enabled"]:
                total_models += len(data.get("models", []))
        
        print(f"Total Models Available: {total_models}")
    else:
        print("\n‚ùå No models discovered. Check API keys and network.")
        
asyncio.run(discover_all_models())
DISCOVER_ALL

# Step 5: Show results
echo -e "\nüìä Discovery Results:"
echo "===================="
cat data/llm_models.json | jq '.providers | to_entries | map(select(.value.enabled == true)) | map({
    provider: .key,
    models: (.value.models | length),
    default: .value.default,
    sample_models: .value.models[:2]
})'

# Step 6: Test routing
echo -e "\nüß™ Testing RouteLLM Routing:"
echo "============================"
docker exec zoe-core python3 << 'TEST_ROUTING'
import json

# Load the discovered models
with open("/app/data/llm_models.json") as f:
    config = json.load(f)

print("Testing routing decisions:\n")

# Simulate routing for different complexities
complexities = {
    "simple": "What is 2+2?",
    "medium": "Explain how neural networks work",
    "complex": "Design a microservices architecture for a banking system"
}

for complexity, query in complexities.items():
    print(f"{complexity.upper()}: {query[:50]}")
    
    # Simple routing logic based on discovered models
    if complexity == "simple" and config["providers"]["ollama"]["enabled"]:
        provider = "ollama"
        model = config["providers"]["ollama"]["models"][0]
    elif complexity == "complex" and config["providers"]["anthropic"]["enabled"]:
        provider = "anthropic"
        model = config["providers"]["anthropic"]["models"][0]
    elif config["providers"]["openai"]["enabled"]:
        provider = "openai"
        model = config["providers"]["openai"]["models"][0]
    else:
        # Use whatever is available
        for p, data in config["providers"].items():
            if data["enabled"]:
                provider = p
                model = data["models"][0]
                break
    
    print(f"  ‚Üí Route to: {provider}/{model}\n")
TEST_ROUTING

echo -e "\n‚úÖ ROUTELLM ACTIVATION COMPLETE!"
echo "================================"
echo ""
echo "Your system now has:"
echo "  ü§ñ All available AI providers discovered and enabled"
echo "  üß† Intelligent routing based on query complexity"
echo "  üìä Dynamic model discovery from actual APIs"
echo ""
echo "To manually trigger discovery again:"
echo "  docker exec zoe-core python3 -c 'from llm_models import LLMModelManager; import asyncio; m = LLMModelManager(); asyncio.run(m.discover_all_models())'"
echo ""
echo "Check the discovered models:"
echo "  cat data/llm_models.json | jq '.providers'"
