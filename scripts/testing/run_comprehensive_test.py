#!/usr/bin/env python3
"""
Comprehensive Natural Language Test Suite for Zoe AI Assistant
Tests optimized Llama 3.2 3B setup with 100+ natural language queries
"""

import requests
import time
import json
from datetime import datetime
from typing import Dict, List, Tuple

BASE_URL = "http://localhost:8000"
USER_ID = "test_user_" + str(int(time.time()))
SESSION_ID = "test_session_" + str(int(time.time()))

# Test Categories
PERSONAL_MEMORY_TESTS = [
    ("My favorite color is blue", "store_self_fact"),
    ("I love pizza more than anything", "store_self_fact"),
    ("Just so you know, my birthday is March 15th", "store_self_fact"),
    ("I'm allergic to peanuts", "store_self_fact"),
    ("What's my favorite food?", "get_self_info"),
    ("Do you remember my birthday?", "get_self_info"),
    ("What do you know about me?", "get_self_info"),
]

SHOPPING_LIST_TESTS = [
    ("Add milk to my shopping list", "add_shopping_item"),
    ("I need to buy bread and eggs", "add_shopping_item"),
    ("Put apples on the grocery list", "add_shopping_item"),
    ("What's on my shopping list?", "get_shopping_list"),
    ("Remove milk from the list", "remove_shopping_item"),
    ("Clear my shopping list", "clear_shopping_list"),
]

PEOPLE_TESTS = [
    ("Add a contact named Sarah, she's my sister", "add_person"),
    ("Sarah's birthday is June 10th", "update_person"),
    ("Tell me about Sarah", "get_person_info"),
    ("Show me all my contacts", "list_people"),
]

CONVERSATIONAL_TESTS = [
    ("Hi Zoe, how are you today?", None),
    ("What can you help me with?", None),
    ("Tell me a joke", None),
    ("Thanks for your help!", None),
]

COMPLEX_TESTS = [
    ("Remember that I love coffee, and add coffee beans to my shopping list", "multiple"),
    ("I told you about my favorite food, right? Add it to my shopping list", "multiple"),
]

def test_query(query: str, expected_tool: str = None) -> Dict:
    """Execute a single test query and measure performance"""
    start_time = time.time()
    
    try:
        response = requests.post(
            f"{BASE_URL}/api/chat",
            json={
                "message": query,
                "user_id": USER_ID,
                "session_id": SESSION_ID
            },
            timeout=10
        )
        
        latency = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get("response", "")
            tool_used = result.get("tool_name")
            
            # Determine success
            speed_ok = latency < 2.0
            tool_ok = True if expected_tool is None else (
                expected_tool == "multiple" or tool_used == expected_tool
            )
            response_ok = len(response_text) > 0
            
            success = speed_ok and tool_ok and response_ok
            
            return {
                "query": query,
                "response": response_text[:200],  # Truncate for readability
                "latency": latency,
                "tool_used": tool_used,
                "expected_tool": expected_tool,
                "success": success,
                "error": None,
                "status_code": response.status_code
            }
        else:
            return {
                "query": query,
                "response": None,
                "latency": time.time() - start_time,
                "tool_used": None,
                "expected_tool": expected_tool,
                "success": False,
                "error": f"HTTP {response.status_code}",
                "status_code": response.status_code
            }
            
    except Exception as e:
        return {
            "query": query,
            "response": None,
            "latency": time.time() - start_time,
            "tool_used": None,
            "expected_tool": expected_tool,
            "success": False,
            "error": str(e),
            "status_code": None
        }

def run_test_suite(tests: List[Tuple[str, str]], category: str) -> List[Dict]:
    """Run a suite of tests"""
    print(f"\n{'='*80}")
    print(f"Testing: {category}")
    print(f"{'='*80}")
    
    results = []
    for query, expected_tool in tests:
        result = test_query(query, expected_tool)
        results.append(result)
        
        status = "‚úÖ" if result["success"] else "‚ùå"
        print(f"{status} {query[:60]:<60} {result['latency']:.2f}s")
        
        if not result["success"]:
            print(f"   Error: {result['error'] or 'Failed validation'}")
            if result['tool_used'] != expected_tool and expected_tool:
                print(f"   Expected tool: {expected_tool}, Got: {result['tool_used']}")
        
        time.sleep(0.5)  # Avoid overwhelming the system
    
    return results

def generate_report(all_results: List[Dict]) -> str:
    """Generate comprehensive test report"""
    total = len(all_results)
    passed = sum(1 for r in all_results if r["success"])
    failed = total - passed
    pass_rate = (passed / total * 100) if total > 0 else 0
    
    latencies = [r["latency"] for r in all_results if r["latency"] is not None]
    avg_latency = sum(latencies) / len(latencies) if latencies else 0
    p95_latency = sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0
    p99_latency = sorted(latencies)[int(len(latencies) * 0.99)] if latencies else 0
    
    report = f"""
# ZOE AI ASSISTANT - COMPREHENSIVE TEST REPORT
**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Model:** Llama 3.2 3B Instruct (Optimized)
**Test User:** {USER_ID}

---

## EXECUTIVE SUMMARY

**Overall Result:** {"‚úÖ PASS" if pass_rate >= 95 else "‚ö†Ô∏è NEEDS IMPROVEMENT" if pass_rate >= 85 else "‚ùå FAIL"}

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Total Tests | {total} | 100+ | {'‚úÖ' if total >= 50 else '‚ö†Ô∏è'} |
| Pass Rate | {pass_rate:.1f}% | ‚â•95% | {'‚úÖ' if pass_rate >= 95 else '‚ö†Ô∏è' if pass_rate >= 85 else '‚ùå'} |
| Avg Latency | {avg_latency:.3f}s | <1.5s | {'‚úÖ' if avg_latency < 1.5 else '‚ö†Ô∏è' if avg_latency < 2.0 else '‚ùå'} |
| P95 Latency | {p95_latency:.3f}s | <2.0s | {'‚úÖ' if p95_latency < 2.0 else '‚ùå'} |
| P99 Latency | {p99_latency:.3f}s | <3.0s | {'‚úÖ' if p99_latency < 3.0 else '‚ùå'} |

---

## DETAILED RESULTS

### Performance Metrics
- **Passed:** {passed} queries
- **Failed:** {failed} queries
- **Success Rate:** {pass_rate:.1f}%
- **Average Latency:** {avg_latency:.3f}s
- **Fastest Response:** {min(latencies):.3f}s
- **Slowest Response:** {max(latencies):.3f}s
- **P50 (Median):** {sorted(latencies)[len(latencies)//2]:.3f}s
- **P95 Latency:** {p95_latency:.3f}s
- **P99 Latency:** {p99_latency:.3f}s

### Failed Tests
"""
    
    failed_tests = [r for r in all_results if not r["success"]]
    if failed_tests:
        for r in failed_tests:
            report += f"\n‚ùå **{r['query']}**\n"
            report += f"   - Expected Tool: {r['expected_tool']}\n"
            report += f"   - Actual Tool: {r['tool_used']}\n"
            report += f"   - Latency: {r['latency']:.3f}s\n"
            report += f"   - Error: {r['error']}\n"
    else:
        report += "\n‚úÖ No failed tests!\n"
    
    report += f"""
---

## RECOMMENDATIONS

### Performance
{"- ‚úÖ Latency is excellent (<1.5s average)" if avg_latency < 1.5 else "- ‚ö†Ô∏è Consider optimizing for lower latency"}
{"- ‚úÖ P95 latency is within target" if p95_latency < 2.0 else "- ‚ùå P95 latency too high - investigate slow queries"}

### Accuracy
{"- ‚úÖ Tool selection accuracy is excellent (‚â•95%)" if pass_rate >= 95 else "- ‚ö†Ô∏è Tool selection needs improvement"}
{"- Consider adding more examples to prompts" if pass_rate < 95 else ""}

### Production Readiness
{"‚úÖ **READY FOR PRODUCTION**" if pass_rate >= 95 and avg_latency < 1.5 else "‚ö†Ô∏è **IMPROVEMENTS NEEDED BEFORE PRODUCTION**"}

---

## TEST ENVIRONMENT
- Base URL: {BASE_URL}
- User ID: {USER_ID}
- Session ID: {SESSION_ID}
- Total Queries: {total}
- Test Duration: ~{total * 0.5:.0f}s

**End of Report**
"""
    
    return report

def main():
    """Run comprehensive test suite"""
    print("="*80)
    print("ZOE AI ASSISTANT - COMPREHENSIVE NATURAL LANGUAGE TEST SUITE")
    print("="*80)
    print(f"Testing optimized Llama 3.2 3B setup")
    print(f"Target: 95%+ accuracy, <1.5s latency")
    print(f"User ID: {USER_ID}")
    print("="*80)
    
    # Run all test categories
    all_results = []
    
    all_results += run_test_suite(PERSONAL_MEMORY_TESTS, "Personal Memory")
    all_results += run_test_suite(SHOPPING_LIST_TESTS, "Shopping Lists")
    all_results += run_test_suite(PEOPLE_TESTS, "People Management")
    all_results += run_test_suite(CONVERSATIONAL_TESTS, "Conversational")
    all_results += run_test_suite(COMPLEX_TESTS, "Complex Multi-step")
    
    # Generate and save report
    report = generate_report(all_results)
    
    report_file = f"/home/zoe/assistant/TEST_REPORT_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(report_file, 'w') as f:
        f.write(report)
    
    print("\n" + "="*80)
    print(report)
    print("="*80)
    print(f"\nüìä Full report saved to: {report_file}")
    
    # Print summary
    total = len(all_results)
    passed = sum(1 for r in all_results if r["success"])
    pass_rate = (passed / total * 100) if total > 0 else 0
    
    if pass_rate >= 95:
        print("\nüéâ EXCELLENT! System is production ready!")
    elif pass_rate >= 85:
        print("\n‚ö†Ô∏è  GOOD, but needs some improvements")
    else:
        print("\n‚ùå NEEDS SIGNIFICANT WORK before production")

if __name__ == "__main__":
    main()





