#!/usr/bin/env python3
"""
Test llama.cpp integration with Zoe
"""
import sys
import os
sys.path.insert(0, '/home/zoe/assistant/services/zoe-core')

import asyncio
from llm_provider import get_llm_provider

async def test_llamacpp():
    print("ğŸ§ª Testing llama.cpp Integration")
    print("=" * 60)
    print()
    
    # Get provider (should auto-detect llamacpp on Jetson)
    print("1ï¸âƒ£ Getting LLM provider...")
    provider = get_llm_provider()
    print(f"âœ… Provider: {provider.__class__.__name__}")
    print()
    
    # Test generation
    print("2ï¸âƒ£ Testing generation...")
    try:
        response = await provider.generate("Say hello in one word", temperature=0.7)
        print(f"âœ… Response: '{response}'")
    except Exception as e:
        print(f"âŒ Generation failed: {e}")
        return False
    print()
    
    # Test streaming
    print("3ï¸âƒ£ Testing streaming...")
    try:
        print("   Stream: ", end="", flush=True)
        async for token in provider.generate_stream("Count to 5", temperature=0.7):
            print(token, end="", flush=True)
        print()
        print("âœ… Streaming works")
    except Exception as e:
        print(f"âŒ Streaming failed: {e}")
        return False
    print()
    
    print("=" * 60)
    print("ğŸ‰ All integration tests passed!")
    return True

if __name__ == "__main__":
    success = asyncio.run(test_llamacpp())
    sys.exit(0 if success else 1)






